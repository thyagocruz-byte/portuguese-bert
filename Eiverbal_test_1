pip install transformers datasets torch evaluate scikit-learn
import transformers
print(transformers.__version__)  # Deve ser 4.0 ou superior
Ele      O
chutou   B-IDIOM
o        I-IDIOM
balde    I-IDIOM
ontem    O
{
  "train": [
    {
      "tokens": ["Ele", "chutou", "o", "balde", "ontem"],
      "labels": ["O", "B-IDIOM", "I-IDIOM", "I-IDIOM", "O"]
    },
    {
      "tokens": ["Maria", "ficou", "de", "olho", "na", "vaga"],
      "labels": ["O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM", "O"]
    },
    {
      "tokens": ["João", "meteu", "o", "pé", "na", "estrada"],
      "labels": ["O", "B-IDIOM", "I-IDIOM", "I-IDIOM", "I-IDIOM", "O"]
    },
    {
      "tokens": ["Eles", "lavaram", "as", "mãos", "sobre", "o", "caso"],
      "labels": ["O", "B-IDIOM", "I-IDIOM", "I-IDIOM", "O", "O", "O"]
    },
    {
      "tokens": ["Você", "está", "fazendo", "tempestade", "em", "copo", "d'água"],
      "labels": ["O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Pedro", "bateu", "as", "botas"],
      "labels": ["O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Ela", "segurou", "vela", "na", "festa"],
      "labels": ["O", "B-IDIOM", "I-IDIOM", "O", "O"]
    },
    {
      "tokens": ["O", "time", "pisou", "na", "bola"],
      "labels": ["O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Ele", "pagou", "o", "pato"],
      "labels": ["O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Estamos", "com", "a", "corda", "no", "pescoço"],
      "labels": ["O", "O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Ana", "deu", "com", "a", "língua", "nos", "dentes"],
      "labels": ["O", "B-IDIOM", "I-IDIOM", "I-IDIOM", "I-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Ele", "ficou", "a", "ver", "navios"],
      "labels": ["O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Ela", "tem", "jogo", "de", "cintura"],
      "labels": ["O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["O", "chefe", "virou", "a", "casaca"],
      "labels": ["O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Eles", "compraram", "gato", "por", "lebre"],
      "labels": ["O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Pare", "de", "encher", "linguiça"],
      "labels": ["O", "O", "B-IDIOM", "I-IDIOM"]
    }
  ],
  "validation": [
    {
      "tokens": ["Ele", "não", "moveu", "uma", "palha"],
      "labels": ["O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Ela", "bateu", "o", "martelo"],
      "labels": ["O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    }
  ],
  "test": [
    {
      "tokens": ["O", "plano", "foi", "por", "água", "abaixo"],
      "labels": ["O", "O", "O", "B-IDIOM", "I-IDIOM", "I-IDIOM"]
    },
    {
      "tokens": ["Ele", "tirou", "o", "corpo", "fora"],
      "labels": ["O", "B-IDIOM", "I-IDIOM", "I-IDIOM", "I-IDIOM"]
    }
  ]
}
from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments

# 1. Carrega o modelo e o tokenizador
model_name = "bert-base-uncased"
model = BertForSequenceClassification.from_pretrained(model_name)
tokenizer = BertTokenizer.from_pretrained(model_name)

# 2. Configura os argumentos de salvamento (AQUI ESTÁ O SEGREDO)
training_args = TrainingArguments(
    output_dir="./meus_checkpoints",   # Onde as etapas serão salvas
    num_train_epochs=3,               # Quantas vezes passará pelos dados
    per_device_train_batch_size=8,    # Ajuste conforme sua GPU (8 ou 16 é comum)
    save_strategy="epoch",            # Salva ao final de cada época
    save_total_limit=2,               # Mantém apenas as 2 últimas etapas no HD
    fp16=True,                        # Deixa o treino mais rápido em GPUs modernas
    logging_steps=100,
)

# 3. Inicializa o Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=seu_dataset_tokenizado, # Substitua pelo seu dataset
    eval_dataset=seu_dataset_validacao,   # Substitua pelo seu dataset
)

# 4. DISPARA O TREINAMENTO
trainer.train()

# 5. Salva o resultado final "limpo"
trainer.save_model("./modelo_final_pronto")
import random
import json

idioms = [
    ["chutar o balde"],
    ["meter o pé na jaca"],
    ["pagar o pato"],
    ["lavar as mãos"],
    ["ficar a ver navios"],
    ["comprar gato por lebre"],
    ["pisar na bola"],
    ["segurar vela"],
    ["encher linguiça"],
    ["bater o martelo"]
]

subjects = ["Ele", "Ela", "João", "Maria", "O professor", "O aluno"]
extras = ["ontem", "na reunião", "sem pensar", "rapidamente", "de novo"]

def tokenize(sentence):
    return sentence.split()

def label(tokens, idiom_tokens):
    labels = ["O"] * len(tokens)
    for i in range(len(tokens)):
        if tokens[i:i+len(idiom_tokens)] == idiom_tokens:
            labels[i] = "B-IDIOM"
            for j in range(1, len(idiom_tokens)):
                labels[i+j] = "I-IDIOM"
    return labels

dataset = []

for _ in range(1000):
    subject = random.choice(subjects)
    idiom = random.choice(idioms)[0]
    extra = random.choice(extras)

    sentence = f"{subject} {idiom} {extra}"
    tokens = tokenize(sentence)
    labels = label(tokens, idiom.split())

    dataset.append({
        "tokens": tokens,
        "labels": labels
    })

random.shuffle(dataset)

train = dataset[:800]
val = dataset[800:900]
test = dataset[900:]

with open("idiom_dataset.json", "w") as f:
    json.dump({
        "train": train,
        "validation": val,
        "test": test
    }, f, ensure_ascii=False, indent=2)
chutou o balde
chutaram o balde
vai chutar o balde
tinha chutado o balde
from datasets import Dataset, DatasetDict

# Converta seu dict para DatasetDict
raw_datasets = DatasetDict({
    "train": Dataset.from_dict(dataset["train"]),
    "validation": Dataset.from_dict(dataset["validation"]),
    "test": Dataset.from_dict(dataset["test"])
})
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("neuralmind/bert-base-portuguese-cased")

label_list = ["O", "B-IDIOM", "I-IDIOM"]  # Ajuste se precisar de mais rótulos
label_to_id = {label: i for i, label in enumerate(label_list)}

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True, padding="max_length", max_length=128)
    labels = []
    for i, label in enumerate(examples["labels"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignorar subwords especiais
            elif word_idx != previous_word_idx:
                label_ids.append(label_to_id[label[word_idx]])
            else:
                label_ids.append(-100)  # Ignorar subwords
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    "neuralmind/bert-base-portuguese-cased",
    num_labels=len(label_list)
)
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    save_strategy="epoch",
    load_best_model_at_end=True
)
import evaluate
import numpy as np

metric = evaluate.load("seqeval")

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)
    true_predictions = [[label_list[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]
    true_labels = [[label_list[l] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]
    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {"f1": results["overall_f1"]}
from transformers import Trainer, DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)
